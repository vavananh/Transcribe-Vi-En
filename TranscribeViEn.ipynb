{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laJ5Ks9kXodi"
      },
      "outputs": [],
      "source": [
        "# CÀI ĐẶT\n",
        "!pip install ffmpeg-python openai-whisper torch torchvision torchaudio\n",
        "!pip install transformers sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORT THƯ VIỆN\n",
        "import torch\n",
        "import os\n",
        "import whisper\n",
        "import subprocess\n",
        "import re\n",
        "# import nltk\n",
        "# from nltk.tokenize import sent_tokenize\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "from datetime import timedelta"
      ],
      "metadata": {
        "id": "dDLR-d29XvRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD MODEL WHISPER\n",
        "model = whisper.load_model(\"large\")"
      ],
      "metadata": {
        "id": "7maxgGK4XxBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # TẢI DỮ LIỆU TOKENIZER\n",
        "# nltk.download('punkt_tab')\n",
        "\n",
        "# TẢI MÔ HÌNH VAD\n",
        "vad_model, utils = torch.hub.load('snakers4/silero-vad', 'silero_vad', trust_repo=True)\n",
        "get_speech_timestamps, save_audio, read_audio, _, _ = utils"
      ],
      "metadata": {
        "id": "uMZZzDPHXzvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TIỀN XỬ LÝ ÂM THANH BẰNG FFMPEG\n",
        "def preprocess_audio(input_path, output_path=\"processed.wav\"):\n",
        "    command = [\"ffmpeg\", \"-y\", \"-i\", input_path, \"-ac\", \"1\", \"-ar\", \"16000\", output_path]\n",
        "    subprocess.run(command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "    return output_path\n",
        "\n",
        "# TÁCH ĐOẠN BẰNG VAD\n",
        "def split_audio_by_vad(input_wav, model, output_dir=\"segments\"):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    audio = read_audio(input_wav, sampling_rate=16000)\n",
        "    speech_timestamps = get_speech_timestamps(audio, model, sampling_rate=16000)\n",
        "    clean_segments = [s for s in speech_timestamps if s['end'] - s['start'] > 1000]\n",
        "\n",
        "    segment_paths = []\n",
        "    for i, segment in enumerate(clean_segments):\n",
        "        segment_path = os.path.join(output_dir, f\"segment_{i}.wav\")\n",
        "        save_audio(segment_path, audio[segment['start']:segment['end']], sampling_rate=16000)\n",
        "        segment_paths.append(segment_path)\n",
        "\n",
        "    return segment_paths, clean_segments\n",
        "\n",
        "# NHẬN DIỆN TIẾNG NÓI (WHISPER)\n",
        "def transcribe_segments(segment_paths, speech_timestamps):\n",
        "    all_text = []\n",
        "    all_segments = []\n",
        "\n",
        "    for i, path in enumerate(segment_paths):\n",
        "        offset_sec = speech_timestamps[i]['start'] / 16000\n",
        "        result = model.transcribe(path, language=\"vi\", word_timestamps=False)\n",
        "        all_text.append(result[\"text\"])\n",
        "\n",
        "        for seg in result[\"segments\"]:\n",
        "            seg[\"start\"] += offset_sec\n",
        "            seg[\"end\"] += offset_sec\n",
        "            all_segments.append(seg)\n",
        "\n",
        "    return \"\\n\".join(all_text), {\"segments\": all_segments}\n",
        "\n",
        "# DỊCH VI -> EN BẰNG MarianMT\n",
        "mt_model_name = \"Helsinki-NLP/opus-mt-vi-en\"\n",
        "tokenizer = MarianTokenizer.from_pretrained(mt_model_name)\n",
        "translator = MarianMTModel.from_pretrained(mt_model_name)\n",
        "\n",
        "# TÁCH CÂU TIẾNG VIỆT\n",
        "def split_sentences(text):\n",
        "    # Tách câu theo dấu kết thúc câu và có thể bắt đầu bằng chữ cái hoa\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-ZÀ-Ỵ])', text)\n",
        "    return [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "def translate_sentences(sentences):\n",
        "    translated = []\n",
        "    for sent in sentences:\n",
        "        inputs = tokenizer(sent, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        outputs = translator.generate(**inputs)\n",
        "        translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        translated.append(translated_text)\n",
        "    return translated\n",
        "\n",
        "# TẠO FILE SONG NGỮ\n",
        "def export_bilingual(sentences_vi, sentences_en, txt_path=\"bilingual_transcript.txt\"):\n",
        "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for vi, en in zip(sentences_vi, sentences_en):\n",
        "            f.write(f\"{vi}\\n{en}\\n\\n\")\n",
        "    print(f\"Đã lưu tại {txt_path}\")"
      ],
      "metadata": {
        "id": "NkKLrPfsX2gK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XUẤT PHỤ ĐỀ\n",
        "def format_timestamp(seconds: float):\n",
        "    delta = timedelta(seconds=seconds)\n",
        "    return str(delta)[:-3].replace('.', ',')\n",
        "\n",
        "def export_srt(transcription_results, output_path=\"output_subtitles.srt\"):\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for i, segment in enumerate(transcription_results[\"segments\"]):\n",
        "            start = format_timestamp(segment[\"start\"])\n",
        "            end = format_timestamp(segment[\"end\"])\n",
        "            text = segment[\"text\"].strip()\n",
        "            f.write(f\"{i+1}\\n{start} --> {end}\\n{text}\\n\\n\")\n",
        "    print(f\"Đã xuất phụ đề tại: {output_path}\")"
      ],
      "metadata": {
        "id": "1vCI1-gPmthc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GHI VĂN BẢN\n",
        "def export_text_output(text, txt_path):\n",
        "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text)\n",
        "    print(f\"Đã lưu văn bản tại: {txt_path}\")"
      ],
      "metadata": {
        "id": "HSIBUTGQgWWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CHẠY CHƯƠNG TRÌNH\n",
        "input_file = \"/content/au1.m4a\"\n",
        "processed = preprocess_audio(input_file)\n",
        "segments, timestamps = split_audio_by_vad(processed, vad_model)\n",
        "text_output, segment_data = transcribe_segments(segments, timestamps)\n",
        "\n",
        "# GHI VĂN BẢN GỐC\n",
        "export_text_output(text_output, \"transcript_vi_full.txt\")\n",
        "\n",
        "# XUẤT SRT\n",
        "export_srt(segment_data, \"transcript_vi.srt\")\n",
        "\n",
        "# CHIA CÂU, DỊCH, VÀ GHI FILE SONG NGỮ\n",
        "sentences_vi = split_sentences(text_output)\n",
        "sentences_en = translate_sentences(sentences_vi)\n",
        "export_bilingual(sentences_vi, sentences_en)\n",
        "\n",
        "# IN RA 5 DÒNG MẪU\n",
        "for vi, en in zip(sentences_vi[:5], sentences_en[:5]):\n",
        "    print(\"- VI:\", vi)\n",
        "    print(\"- EN:\", en)\n",
        "    print()"
      ],
      "metadata": {
        "id": "BwqJMoO2X4i9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NHẬN DIỆN BẰNG WHISPER TRỰC TIẾP KHÔNG VAD, KHÔNG CHUẨN HÓA ÂM THANH\n",
        "\n",
        "result = model.transcribe(input_file, language=\"vi\")\n",
        "text_output = result[\"text\"]\n",
        "\n",
        "print(\"Văn bản nhận diện:\")\n",
        "print(text_output)\n",
        "\n",
        "export_text_output(text_output, \"raw_transcript.txt\")"
      ],
      "metadata": {
        "id": "PV73r6lfX7Ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NHẬN DIỆN BẰNG WHISPER CÓ VAD, CÓ CHUẨN HÓA ÂM THANH\n",
        "\n",
        "processed = preprocess_audio(input_file)\n",
        "segments, timestamps = split_audio_by_vad(processed, vad_model)\n",
        "text_output, segment_data = transcribe_segments(segments, timestamps)\n",
        "\n",
        "print(\"Văn bản nhận diện:\")\n",
        "print(text_output)\n",
        "\n",
        "export_text_output(text_output, \"whisper_vad.txt\")"
      ],
      "metadata": {
        "id": "ZUQg-TRtcTjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NHẬN DIỆN BẰNG WHISPER KHÔNG VAD, CÓ CHUẨN HÓA ÂM THANH\n",
        "\n",
        "processed = preprocess_audio(input_file)\n",
        "result = model.transcribe(processed, language=\"vi\")\n",
        "text_output = result[\"text\"]\n",
        "\n",
        "print(\"Văn bản nhận diện:\")\n",
        "print(text_output)\n",
        "\n",
        "export_text_output(text_output, \"whisper_no_vad.txt\")"
      ],
      "metadata": {
        "id": "CoySF9mkMy4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn nest_asyncio pyngrok python-multipart"
      ],
      "metadata": {
        "id": "wjLnLNXCc33C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "from fastapi import FastAPI, File, UploadFile\n",
        "from fastapi.responses import FileResponse\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok, conf\n",
        "import uvicorn\n",
        "import threading\n",
        "\n",
        "# Ngắt hết tunnel hiện có\n",
        "for tunnel in ngrok.get_tunnels():\n",
        "    ngrok.disconnect(tunnel.public_url)\n",
        "\n",
        "# Apply nest_asyncio để chạy uvicorn\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Khởi tạo FastAPI\n",
        "app = FastAPI()\n",
        "\n",
        "# Thư mục lưu file upload + kết quả\n",
        "UPLOAD_DIR = \"/content/uploads\"\n",
        "RESULT_DIR = \"/content/results\"\n",
        "os.makedirs(UPLOAD_DIR, exist_ok=True)\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "\n",
        "# Biến global lưu URL public ngrok\n",
        "public_url = None\n",
        "\n",
        "@app.post(\"/transcribe/\")\n",
        "async def transcribe_audio(file: UploadFile = File(...)):\n",
        "    # Lưu file upload\n",
        "    upload_path = os.path.join(UPLOAD_DIR, file.filename)\n",
        "    with open(upload_path, \"wb\") as buffer:\n",
        "        shutil.copyfileobj(file.file, buffer)\n",
        "\n",
        "    # Gọi lại các hàm xử lý\n",
        "    processed_path = preprocess_audio(upload_path, output_path=os.path.join(UPLOAD_DIR, \"processed.wav\"))\n",
        "    segments = split_audio_by_vad(processed_path, vad_model, output_dir=os.path.join(UPLOAD_DIR, \"segments\"))\n",
        "    text_output, segment_data = transcribe_segments(segments)\n",
        "\n",
        "    base_name = file.filename.rsplit('.', 1)[0]\n",
        "    vi_txt = os.path.join(RESULT_DIR, f\"transcript_vi_{base_name}.txt\")\n",
        "    srt_file = os.path.join(RESULT_DIR, f\"transcript_vi_{base_name}.srt\")\n",
        "    bilingual_txt = os.path.join(RESULT_DIR, f\"bilingual_transcript_{base_name}.txt\")\n",
        "\n",
        "    export_text_output(text_output, vi_txt)\n",
        "    export_srt(segment_data, srt_file)\n",
        "    sentences_vi = split_sentences(text_output)\n",
        "    sentences_en = translate_sentences(sentences_vi)\n",
        "    export_bilingual(sentences_vi, sentences_en, bilingual_txt)\n",
        "\n",
        "    return {\n",
        "        \"message\": \"Xử lý thành công!\",\n",
        "        \"preview_text\": text_output[:300],\n",
        "        \"download_links\": {\n",
        "            \"transcript_vi\": f\"{public_url}/download/{os.path.basename(vi_txt)}\",\n",
        "            \"subtitles_srt\": f\"{public_url}/download/{os.path.basename(srt_file)}\",\n",
        "            \"bilingual_txt\": f\"{public_url}/download/{os.path.basename(bilingual_txt)}\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "@app.get(\"/download/{filename}\")\n",
        "async def download_file(filename: str):\n",
        "    # Tìm file trong thư mục kết quả và trả về\n",
        "    for dir_ in [RESULT_DIR, UPLOAD_DIR]:\n",
        "        file_path = os.path.join(dir_, filename)\n",
        "        if os.path.exists(file_path):\n",
        "            return FileResponse(file_path, filename=filename)\n",
        "    return {\"error\": \"File không tồn tại\"}\n",
        "\n",
        "# Khởi chạy server trong luồng background\n",
        "def run():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "# Thêm token ngrok\n",
        "NGROK_TOKEN = \"2xfBr6SgqdP0T5h6D1yoXzWhnCy_2im2NSeh4rEJdJZJaK66S\"\n",
        "ngrok.set_auth_token(NGROK_TOKEN)\n",
        "public_url = ngrok.connect(8000).public_url\n",
        "\n",
        "print(f\"Server đang chạy tại: {public_url}\")\n",
        "print(f\"API docs: {public_url}/docs\")\n",
        "\n",
        "threading.Thread(target=run, daemon=True).start()"
      ],
      "metadata": {
        "id": "wIOR5JOUBHV3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}